// 20190620081637
// http://sparkapplicationcomposeralb-822020478.us-east-1.elb.amazonaws.com:8080/api/v1/applications

{
    "applications": [
      {
        "_id": "5cab8ed9091ac2c19329dcc1",
        "name": "test",
        "globals": {
          
        },
        "executions": [
          {
            "id": "test a",
            "pipelineIds": [
              "b0318ed0-57d1-11e9-85d2-29bb75bc15e9"
            ],
            "pipelines": [
              
            ]
          },
          {
            "id": "test b",
            "pipelineIds": [
              "6f399d30-57d8-11e9-85d2-29bb75bc15e9"
            ],
            "pipelines": [
              
            ],
            "parents": [
              "test a"
            ]
          }
        ],
        "pipelines": [
          {
            "_id": "5ca7a01cab4f68476b49be6e",
            "name": "test",
            "steps": [
              {
                "id": "load test",
                "displayName": "Load DataFrame from HDFS path",
                "description": "This step will read a dataFrame from the given HDFS path",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "path",
                    "required": false,
                    "value": "!inFile"
                  },
                  {
                    "type": "object",
                    "name": "options",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameReaderOptions"
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.readFromPath"
                },
                "creationDate": "2019-04-04T14:24:22.875Z",
                "modifiedDate": "2019-04-04T14:24:22.875Z",
                "stepId": "87db259d-606e-46eb-b723-82923349640f"
              }
            ],
            "id": "b0318ed0-57d1-11e9-85d2-29bb75bc15e9",
            "creationDate": "2019-04-05T18:36:12.739Z",
            "modifiedDate": "2019-04-05T18:37:07.082Z"
          },
          {
            "_id": "5ca7ab6eab4f683c6849be6f",
            "name": "test2",
            "steps": [
              {
                "id": "test load",
                "displayName": "Load DataFrame from HDFS path",
                "description": "This step will read a dataFrame from the given HDFS path",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "path",
                    "required": false,
                    "value": "!inPath"
                  },
                  {
                    "type": "object",
                    "name": "options",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameReaderOptions"
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.readFromPath"
                },
                "creationDate": "2019-04-04T14:24:22.875Z",
                "modifiedDate": "2019-04-04T14:24:22.875Z",
                "stepId": "87db259d-606e-46eb-b723-82923349640f"
              }
            ],
            "id": "6f399d30-57d8-11e9-85d2-29bb75bc15e9",
            "creationDate": "2019-04-05T19:24:30.214Z",
            "modifiedDate": "2019-04-05T19:24:30.214Z"
          }
        ],
        "id": "c03f6590-5a29-11e9-aa07-a58054497ebb",
        "creationDate": "2019-04-08T18:11:37.708Z",
        "modifiedDate": "2019-04-08T20:57:29.988Z"
      },
      {
        "_id": "5cabb5fbb618a23f5c323cec",
        "name": "test2",
        "globals": {
          
        },
        "executions": [
          {
            "id": "test a",
            "pipelineIds": [
              "b0318ed0-57d1-11e9-85d2-29bb75bc15e9"
            ],
            "pipelines": [
              
            ],
            "globals": {
              
            }
          },
          {
            "id": "test b",
            "pipelineIds": [
              "6f399d30-57d8-11e9-85d2-29bb75bc15e9"
            ],
            "pipelines": [
              
            ],
            "parents": [
              "test a"
            ],
            "globals": {
              
            }
          },
          {
            "id": "test c",
            "pipelineIds": [
              "7fef3fb0-5d30-11e9-b761-a9a8d6eac9c4"
            ],
            "pipelines": [
              
            ],
            "parents": [
              "test a"
            ],
            "globals": {
              
            }
          }
        ],
        "pipelines": [
          {
            "_id": "5ca7a01cab4f68476b49be6e",
            "name": "test",
            "steps": [
              {
                "id": "load test",
                "displayName": "Load DataFrame from HDFS path",
                "description": "This step will read a dataFrame from the given HDFS path",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "path",
                    "required": false,
                    "value": "!inFile"
                  },
                  {
                    "type": "object",
                    "name": "options",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameReaderOptions"
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.readFromPath"
                },
                "creationDate": "2019-04-04T14:24:22.875Z",
                "modifiedDate": "2019-04-04T14:24:22.875Z",
                "stepId": "87db259d-606e-46eb-b723-82923349640f"
              }
            ],
            "id": "b0318ed0-57d1-11e9-85d2-29bb75bc15e9",
            "creationDate": "2019-04-05T18:36:12.739Z",
            "modifiedDate": "2019-04-05T18:37:07.082Z"
          },
          {
            "_id": "5ca7ab6eab4f683c6849be6f",
            "name": "test2",
            "steps": [
              {
                "id": "test load",
                "displayName": "Load DataFrame from HDFS path",
                "description": "This step will read a dataFrame from the given HDFS path",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "path",
                    "required": false,
                    "value": "!inPath"
                  },
                  {
                    "type": "object",
                    "name": "options",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameReaderOptions"
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.readFromPath"
                },
                "creationDate": "2019-04-04T14:24:22.875Z",
                "modifiedDate": "2019-04-04T14:24:22.875Z",
                "stepId": "87db259d-606e-46eb-b723-82923349640f"
              }
            ],
            "id": "6f399d30-57d8-11e9-85d2-29bb75bc15e9",
            "creationDate": "2019-04-05T19:24:30.214Z",
            "modifiedDate": "2019-04-05T19:24:30.214Z"
          },
          {
            "_id": "5cb0a2a9607415226feb6229",
            "name": "Aggies",
            "steps": [
              {
                "id": "Load",
                "displayName": "Load DataFrame from HDFS path",
                "description": "This step will read a dataFrame from the given HDFS path",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "path",
                    "type": "text",
                    "value": "/landing/test.csv",
                    "required": false
                  },
                  {
                    "type": "object",
                    "name": "options",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameReaderOptions"
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.readFromPath"
                },
                "creationDate": "2019-04-04T14:24:22.875Z",
                "modifiedDate": "2019-04-04T14:24:22.875Z",
                "stepId": "87db259d-606e-46eb-b723-82923349640f",
                "nextStepId": "Filter"
              },
              {
                "id": "Filter",
                "displayName": "Filter a DataFrame",
                "description": "This step will filter a dataframe based on the where expression provided",
                "type": "Pipeline",
                "category": "Transforms",
                "params": [
                  {
                    "type": "text",
                    "name": "dataFrame",
                    "required": false,
                    "value": "@Load"
                  },
                  {
                    "name": "expression",
                    "type": "text",
                    "value": "",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "TransformationSteps.applyFilter"
                },
                "creationDate": "2019-04-04T14:24:22.976Z",
                "modifiedDate": "2019-04-04T14:24:22.976Z",
                "stepId": "fa0fcabb-d000-4a5e-9144-692bca618ddb",
                "nextStepId": "Write"
              },
              {
                "id": "Write",
                "displayName": "Write DataFrame to HDFS",
                "description": "This step will write a dataFrame in a given format to HDFS",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "dataFrame",
                    "type": "text",
                    "value": "@Filter",
                    "required": false
                  },
                  {
                    "name": "path",
                    "type": "text",
                    "value": "/raw/my_parquet",
                    "required": false
                  },
                  {
                    "name": "options",
                    "type": "object",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameWriterOptions"
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.writeToPath"
                },
                "creationDate": "2019-04-04T14:24:22.889Z",
                "modifiedDate": "2019-04-04T14:24:22.889Z",
                "stepId": "0a296858-e8b7-43dd-9f55-88d00a7cd8fa"
              }
            ],
            "id": "7fef3fb0-5d30-11e9-b761-a9a8d6eac9c4",
            "creationDate": "2019-04-12T14:37:29.774Z",
            "modifiedDate": "2019-04-12T14:37:29.774Z"
          }
        ],
        "id": "1308aae0-5a41-11e9-b4e4-75b75d3f3fac",
        "creationDate": "2019-04-08T20:58:35.030Z",
        "modifiedDate": "2019-04-12T14:40:18.578Z"
      },
      {
        "_id": "5ced8f787b60af34eb1ffd3e",
        "name": "ebalog_download",
        "globals": {
          
        },
        "executions": [
          {
            "id": "parquet",
            "pipelineIds": [
              "0129ee10-8633-11e9-ac99-49159f6e60a9"
            ],
            "pipelines": [
              
            ],
            "globals": {
              
            }
          }
        ],
        "pipelines": [
          {
            "_id": "5cf5703b8c5e443e54933eba",
            "name": "ebalog_parquet",
            "steps": [
              {
                "id": "SFTP_Manager",
                "displayName": "Create SFTP FileManager",
                "description": "Simple function to generate the SFTPFileManager for the remote SFTP file system",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "hostName",
                    "type": "text",
                    "value": "!sftp_host",
                    "required": false
                  },
                  {
                    "name": "username",
                    "type": "text",
                    "value": "!sftp_user",
                    "required": false
                  },
                  {
                    "name": "password",
                    "type": "text",
                    "value": "!sftp_pass",
                    "required": false
                  },
                  {
                    "name": "port",
                    "type": "integer",
                    "value": "!sftp_port",
                    "required": false
                  },
                  {
                    "name": "strictHostChecking",
                    "type": "boolean",
                    "value": "false",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "SFTPSteps.createFileManager",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T19:10:59.281Z",
                "modifiedDate": "2019-06-07T00:10:31.084Z",
                "stepId": "9d467cb0-8b3d-40a0-9ccd-9cf8c5b6cb38",
                "nextStepId": "HDFS_Manager"
              },
              {
                "id": "HDFS_Manager",
                "displayName": "Create HDFS FileManager",
                "description": "Simple function to generate the HDFSFileManager for the local HDFS file system",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.createFileManager",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T18:54:44.836Z",
                "modifiedDate": "2019-05-28T18:54:44.836Z",
                "stepId": "e4dad367-a506-5afd-86c0-82c2cf5cd15c",
                "nextStepId": "Download"
              },
              {
                "id": "Download",
                "displayName": "Buffered file copy",
                "description": "Copy the contents of the source path to the destination path using full buffer sizes. This function will call connect on both FileManagers.",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "srcFS",
                    "type": "text",
                    "value": "@SFTP_FileManager",
                    "required": false
                  },
                  {
                    "name": "srcPath",
                    "type": "text",
                    "value": "!sftp_input_path",
                    "required": false
                  },
                  {
                    "name": "destFS",
                    "type": "text",
                    "value": "@HDFS_FileManager",
                    "required": false
                  },
                  {
                    "name": "destPath",
                    "type": "text",
                    "value": "!landing_path",
                    "required": false
                  },
                  {
                    "type": "integer",
                    "name": "inputBufferSize",
                    "required": false,
                    "value": "!input_buffer_size"
                  },
                  {
                    "name": "outputBufferSize",
                    "type": "integer",
                    "value": "!output_buffer_size",
                    "required": false
                  },
                  {
                    "name": "copyBufferSize",
                    "type": "integer",
                    "value": "!read_buffer_size",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "FileManagerSteps.copy",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T18:54:44.883Z",
                "modifiedDate": "2019-06-07T00:11:00.761Z",
                "stepId": "f5a24db0-e91b-5c88-8e67-ab5cff09c883",
                "nextStepId": "DisconnectSFTP"
              },
              {
                "id": "DisconnectSFTP",
                "displayName": "Disconnect a FileManager",
                "description": "Disconnects a FileManager from the underlying file system",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "fileManager",
                    "type": "text",
                    "value": "@SFTP_FileManager",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "FileManagerSteps.disconnectFileManager",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T18:54:44.887Z",
                "modifiedDate": "2019-05-28T18:54:44.887Z",
                "stepId": "3d1e8519-690c-55f0-bd05-1e7b97fb6633",
                "nextStepId": "LoadLandingFile"
              },
              {
                "id": "LoadLandingFile",
                "displayName": "Load DataFrame from HDFS path",
                "description": "This step will read a dataFrame from the given HDFS path",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "path",
                    "type": "text",
                    "value": "!landing_path",
                    "required": false
                  },
                  {
                    "name": "options",
                    "type": "object",
                    "value": {
                      "format": "csv",
                      "options": {
                        "header": "true",
                        "delimiter": "|"
                      },
                      "schema": {
                        "attributes": [
                          
                        ]
                      }
                    },
                    "className": "com.acxiom.pipeline.steps.DataFrameReaderOptions",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.readFromPath"
                },
                "creationDate": "2019-04-04T14:24:22.875Z",
                "modifiedDate": "2019-04-04T14:24:22.875Z",
                "stepId": "87db259d-606e-46eb-b723-82923349640f",
                "executeIfEmpty": "",
                "nextStepId": "SplitDataFrame"
              },
              {
                "id": "SplitDataFrame",
                "displayName": "Custom Step to Split Dataframe Based on Input Size",
                "description": "This step re-splits dataframe based on file size to get close to 128M per split",
                "type": "Pipeline",
                "category": "EdsStuff",
                "params": [
                  {
                    "name": "dataFrame",
                    "type": "text",
                    "value": "@LoadLandingFile",
                    "required": false
                  },
                  {
                    "name": "hdfsFM",
                    "type": "text",
                    "value": "@HDFSFileManager",
                    "required": false
                  },
                  {
                    "name": "path",
                    "type": "text",
                    "value": "!landing_path",
                    "required": false
                  },
                  {
                    "name": "splitMode",
                    "type": "text",
                    "value": "!split_mode",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "EdSteps.splitDataFrameOnFileSize",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-06-03T20:31:40.580Z",
                "modifiedDate": "2019-06-03T20:31:40.580Z",
                "stepId": "ea4ffd1e-0066-4bde-9c92-1e34db61b9b3",
                "nextStepId": "WriteToParquet"
              },
              {
                "id": "WriteToParquet",
                "displayName": "Custom Write a DataFrame to Parquet DataStore with Metrics",
                "description": "This step will write a dataFrame to a Parquet data store and return run time metrics",
                "type": "Pipeline",
                "category": "EdsStuff",
                "params": [
                  {
                    "name": "dataFrame",
                    "type": "text",
                    "value": "@SplitDataFrame",
                    "required": false
                  },
                  {
                    "name": "hdfsFileManager",
                    "type": "text",
                    "value": "@HDFSFileManager",
                    "required": false
                  },
                  {
                    "name": "path",
                    "type": "text",
                    "value": "!parquet_path",
                    "required": false
                  },
                  {
                    "name": "splitMode",
                    "type": "text",
                    "value": "!split_mode",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "EdSteps.writeToParquet",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-06-03T20:38:14.661Z",
                "modifiedDate": "2019-06-03T20:39:02.547Z",
                "stepId": "6fefd9f0-4c20-47e7-a655-ad203683d9c3",
                "nextStepId": "SaveResults"
              },
              {
                "id": "SaveResults",
                "displayName": "Custom Step to Compile Parquet Research Results",
                "description": "This step compiles results from a copy step to be enhanced and written to disk",
                "type": "Pipeline",
                "category": "EdsStuff",
                "params": [
                  {
                    "name": "results",
                    "type": "object",
                    "value": "@WriteToParquet",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.CopyResults"
                  },
                  {
                    "type": "object",
                    "name": "dlResults",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.CopyResults"
                  }
                ],
                "engineMeta": {
                  "spark": "EdSteps.writeParquetResultsToFile",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-06-03T20:31:40.563Z",
                "modifiedDate": "2019-06-07T00:03:22.701Z",
                "stepId": "fa4803a1-e705-4cc8-988e-ba981f4335cd",
                "executeIfEmpty": ""
              }
            ],
            "id": "0129ee10-8633-11e9-ac99-49159f6e60a9",
            "creationDate": "2019-06-03T19:08:43.252Z",
            "modifiedDate": "2019-06-07T00:11:53.202Z"
          }
        ],
        "id": "ec273420-8180-11e9-8b4f-9b89e27b8b39",
        "creationDate": "2019-05-28T19:43:52.934Z",
        "modifiedDate": "2019-06-07T00:12:44.350Z",
        "sparkConf": {
          "kryoClasses": [
            "org.apache.hadoop.io.LongWritable",
            "org.apache.http.client.entity.UrlEncodedFormEntity"
          ],
          "setOptions": [
            {
              "name": "spark.hadoop.io.compression.codecs",
              "value": "org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.Lz4Codec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.GzipCodec"
            },
            {
              "name": "applicationConfigurationLoader",
              "value": "com.acxiom.pipeline.fs.HDFSFileManager"
            }
          ]
        },
        "stepPackages": [
          "com.acxiom.pipeline.steps",
          "com.acxiom.pipeline"
        ],
        "requiredParameters": [
          "sftp_input_path",
          "landing_path",
          "read_buffer_size",
          "input_buffer_size",
          "output_buffer_size"
        ]
      },
      {
        "_id": "5cf875ab7b60afb8f41ffd83",
        "name": "Original Inbound",
        "globals": {
          
        },
        "executions": [
          {
            "id": "Ingest to Raw",
            "pipelineIds": [
              "ff8428d0-879c-11e9-8b4f-9b89e27b8b39"
            ],
            "pipelines": [
              
            ],
            "globals": {
              
            }
          }
        ],
        "pipelines": [
          {
            "_id": "5cf7cf8e7b60aff13a1ffd82",
            "name": "Ingest to Raw Zone",
            "steps": [
              {
                "id": "SetStatusForDownload",
                "displayName": "Update Status of an Existing DataCollection",
                "description": "This step will update the status of a data collection by calling the data collection patch endpoint",
                "type": "Pipeline",
                "category": "ApiInteraction",
                "params": [
                  {
                    "name": "status",
                    "type": "text",
                    "value": "\"Processing\"",
                    "required": false
                  },
                  {
                    "name": "detail",
                    "type": "text",
                    "value": "\"Begin Download from External Site\"",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataCollectionSteps.registerActivity",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.085Z",
                "modifiedDate": "2019-06-04T19:45:30.085Z",
                "stepId": "7f797630-753f-586d-8836-9b500b00c49f",
                "nextStepId": "DownloadFile"
              },
              {
                "id": "DownloadFile",
                "displayName": "Downloads a File",
                "description": "This step downloads a file using the provided data asset and data collection objects",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "dataAsset",
                    "required": false,
                    "value": "!landing_asset"
                  },
                  {
                    "name": "dataCollection",
                    "type": "text",
                    "value": "!dataCollection",
                    "required": false
                  },
                  {
                    "name": "encryptOnWrite",
                    "type": "text",
                    "value": "$encryptOnWrite || false",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "FileSteps.downloadFile",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.150Z",
                "modifiedDate": "2019-06-04T19:45:30.150Z",
                "stepId": "8d8e99b0-b2da-5f19-8e3d-78eefe614392",
                "nextStepId": "SetStatusDecrypting"
              },
              {
                "id": "SetStatusDecrypting",
                "displayName": "Update Status of an Existing DataCollection",
                "description": "This step will update the status of a data collection by calling the data collection patch endpoint",
                "type": "Pipeline",
                "category": "ApiInteraction",
                "params": [
                  {
                    "name": "status",
                    "type": "text",
                    "value": "\"Processing\"",
                    "required": false
                  },
                  {
                    "name": "detail",
                    "type": "text",
                    "value": "\"Encryptioning/Decrypting Landing Asset\"",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataCollectionSteps.registerActivity",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.085Z",
                "modifiedDate": "2019-06-04T19:45:30.085Z",
                "stepId": "7f797630-753f-586d-8836-9b500b00c49f",
                "nextStepId": "HandleEncryptionOnLandingFile"
              },
              {
                "id": "HandleEncryptionOnLandingFile",
                "displayName": "Decrypts DataAsset",
                "description": "This step gets decrypted DataAsset and persists in on file system with encryption if not already encrypted",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "dataAsset",
                    "type": "text",
                    "value": "!landing_asset",
                    "required": false
                  },
                  {
                    "type": "text",
                    "name": "feed",
                    "required": false,
                    "value": "!feed"
                  }
                ],
                "engineMeta": {
                  "spark": "FileSteps.decryptDataAsset",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.173Z",
                "modifiedDate": "2019-06-04T19:45:30.173Z",
                "stepId": "4d828bd2-d8fe-5574-a374-202a28e8554d",
                "nextStepId": "StatusLoadingFile"
              },
              {
                "id": "StatusLoadingFile",
                "displayName": "Update Status of an Existing DataCollection",
                "description": "This step will update the status of a data collection by calling the data collection patch endpoint",
                "type": "Pipeline",
                "category": "ApiInteraction",
                "params": [
                  {
                    "name": "status",
                    "type": "text",
                    "value": "\"Processing\"",
                    "required": false
                  },
                  {
                    "name": "detail",
                    "type": "text",
                    "value": "\"Loading Landing File to Raw Zone\"",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataCollectionSteps.registerActivity",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.085Z",
                "modifiedDate": "2019-06-04T19:45:30.085Z",
                "stepId": "7f797630-753f-586d-8836-9b500b00c49f",
                "nextStepId": "CheckForDuplicateCollections"
              },
              {
                "id": "CheckForDuplicateCollections",
                "displayName": "Check for Potential Duplicate Collections",
                "description": "This step determines if the current data collection is a possible duplicate of another",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "encryptedDataAsset",
                    "type": "text",
                    "value": "#HandleEncryptionOnLandingFile.encryptedDataAsset",
                    "required": false
                  },
                  {
                    "name": "unencryptedDataAsset",
                    "type": "text",
                    "value": "@HandleEncryptionOnLandingFile",
                    "required": false
                  },
                  {
                    "name": "feed",
                    "type": "text",
                    "value": "!feed",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "FileSteps.checkDuplicateFile",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.178Z",
                "modifiedDate": "2019-06-04T19:45:30.178Z",
                "stepId": "1db3ae12-9b30-5d42-a596-cae8df3302a9",
                "nextStepId": "ValidateMimeType"
              },
              {
                "id": "ValidateMimeType",
                "displayName": "Validate Mime Type of a File DataAsset",
                "description": "This step validates the mime type of a file stored in a data asset",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "dataAsset",
                    "type": "text",
                    "value": "@HandleEncryptionOnLandingFile",
                    "required": false
                  },
                  {
                    "name": "supportedMimeTypes",
                    "type": "text",
                    "value": "Seq(\"text\")",
                    "language": "scala",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "FileSteps.validateSupportedMimeType",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.188Z",
                "modifiedDate": "2019-06-04T19:45:30.188Z",
                "stepId": "7663c5ce-a904-4d4f-aca5-5660317ec3af",
                "nextStepId": "GenerateParsingOptions"
              },
              {
                "id": "GenerateParsingOptions",
                "displayName": "Generate File Parsing Options for a DataAsset",
                "description": "This step evaluates data for a data asset to determine the file parsing options",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "dataAsset",
                    "type": "text",
                    "value": "@HandleEncryptionOnLandingFile",
                    "required": false
                  },
                  {
                    "name": "recordDelimiter",
                    "type": "text",
                    "value": "!feed.input.file.recordDelimiter",
                    "required": false
                  },
                  {
                    "type": "text",
                    "name": "fieldDelimiter",
                    "required": false,
                    "value": "!feed.input.file.fieldDelimiter"
                  },
                  {
                    "name": "fieldEnclosing",
                    "type": "text",
                    "value": "!feed.input.file.fieldEnclosing",
                    "required": false
                  },
                  {
                    "name": "characterSet",
                    "type": "text",
                    "value": "!feed.input.file.characterSet",
                    "required": false
                  },
                  {
                    "name": "skipRecords",
                    "type": "text",
                    "value": "!feed.input.file.skipRecords",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "FileSteps.generateFileParsingOptions",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.162Z",
                "modifiedDate": "2019-06-04T19:45:30.162Z",
                "stepId": "5433f741-1f48-498f-a497-c1218866b7e3",
                "nextStepId": "LoadLandingAsset"
              },
              {
                "id": "LoadLandingAsset",
                "displayName": "Loads a DataAsset to a DataFrame Using FileParsingOptions",
                "description": "This step loads a data asset to a data frame using file parsing options provided",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "dataAsset",
                    "type": "text",
                    "value": "@HandleEncryptionOnLandingFile",
                    "required": false
                  },
                  {
                    "name": "parsingOptions",
                    "type": "text",
                    "value": "@GenerateParsingOptions",
                    "required": false
                  },
                  {
                    "name": "schema",
                    "type": "text",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "FileSteps.loadInboundDataAsset",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.157Z",
                "modifiedDate": "2019-06-04T19:45:30.157Z",
                "stepId": "d7296207-2367-58b7-9b3e-c8ef373ab91f",
                "executeIfEmpty": "",
                "nextStepId": "AddDataCollectionId"
              },
              {
                "id": "AddDataCollectionId",
                "displayName": "Add a Column with a Static Value to All Rows in a DataFrame",
                "description": "This step will add a column with a static value to all rows in the provided data frame",
                "type": "Pipeline",
                "category": "Utilities",
                "params": [
                  {
                    "name": "dataFrame",
                    "type": "text",
                    "value": "@LoadLandingAsset",
                    "required": false
                  },
                  {
                    "name": "columnName",
                    "type": "text",
                    "value": "data_collection_id",
                    "required": false
                  },
                  {
                    "name": "columnValue",
                    "type": "text",
                    "value": "!dataCollectionId",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataSteps.addStaticColumn",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.013Z",
                "modifiedDate": "2019-06-04T19:45:30.013Z",
                "stepId": "37e10488-02c1-5c85-b47a-efecf681fdd4",
                "nextStepId": "AddUniqueRecordId"
              },
              {
                "id": "AddUniqueRecordId",
                "displayName": "Adds a Unique Identifier to a DataFrame",
                "description": "This step will add a new unique identifier to an existing data frame",
                "type": "Pipeline",
                "category": "Utilities",
                "params": [
                  {
                    "name": "idColumnName",
                    "type": "text",
                    "value": "record_id",
                    "required": false
                  },
                  {
                    "name": "dataFrame",
                    "type": "text",
                    "value": "!AddDataCollectionId",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataSteps.prependUniqueId",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.021Z",
                "modifiedDate": "2019-06-04T19:45:30.021Z",
                "stepId": "9f7d84b0-ebab-57da-8b39-be4c47028242",
                "nextStepId": "GetOrCreateRawDataAsset"
              },
              {
                "id": "GetOrCreateRawDataAsset",
                "displayName": "Get or Create a DataAsset using Attributes",
                "description": "This step will get an asset by name and create a new one if it doesn\\'t exist, using individual attributes",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "assetName",
                    "type": "text",
                    "value": "!dataCollectionId",
                    "required": false
                  },
                  {
                    "name": "parentAsset",
                    "type": "text",
                    "value": "!rawZone",
                    "required": false
                  },
                  {
                    "name": "dataFrame",
                    "type": "text",
                    "value": "@AddUniqueRecordId",
                    "required": false
                  },
                  {
                    "name": "partitionKey",
                    "type": "text",
                    "required": false
                  },
                  {
                    "type": "text",
                    "name": "partitionStrategy",
                    "required": false
                  },
                  {
                    "name": "mergeStrategy",
                    "type": "text",
                    "value": "OVERWRITE",
                    "required": false
                  },
                  {
                    "name": "dataFormat",
                    "type": "text",
                    "value": "parquet",
                    "required": false
                  },
                  {
                    "name": "tempFlag",
                    "type": "text",
                    "value": "false",
                    "required": false
                  },
                  {
                    "type": "text",
                    "name": "existingSchema",
                    "required": false
                  },
                  {
                    "name": "useAssetIdForName",
                    "type": "text",
                    "value": "true",
                    "required": false
                  },
                  {
                    "name": "useAttributeIdsForColumnNames",
                    "type": "text",
                    "value": "false",
                    "required": false
                  },
                  {
                    "name": "update",
                    "type": "text",
                    "value": "true",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataAssetSteps.getOrCreateDataStoreAsset",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:31.086Z",
                "modifiedDate": "2019-06-04T19:45:31.086Z",
                "stepId": "ea28ebc8-09b3-4a89-a670-92915bb8a8f4",
                "nextStepId": "WriateDataToRawZone"
              },
              {
                "id": "WriateDataToRawZone",
                "displayName": "Writes a DataFrame to an Existing DataAsset",
                "description": "This step maps and writes a dataframe to an existing data asset",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "dataFrame",
                    "type": "text",
                    "value": "@AddUniqueRecordId",
                    "required": false
                  },
                  {
                    "type": "text",
                    "name": "dataAsset",
                    "required": false,
                    "value": "@GetOrCreateRawDataAsset"
                  },
                  {
                    "name": "feed",
                    "type": "text",
                    "value": "!feed",
                    "required": false
                  },
                  {
                    "name": "optimalSplitSizeMB",
                    "type": "text",
                    "value": "$optimalSplitSizeMB || 128",
                    "required": false
                  },
                  {
                    "name": "mergeStrategyOverride",
                    "type": "text",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataSteps.writeDataFrameToDataAsset",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:30.052Z",
                "modifiedDate": "2019-06-04T19:45:30.052Z",
                "stepId": "4c8cb4d9-51db-4b7f-b199-7b13559ecf8d",
                "nextStepId": "TagRawDataAsset"
              },
              {
                "id": "TagRawDataAsset",
                "displayName": "Apply DataGroup Tags to an existing DataAsset",
                "description": "This function will apply tags to the final asset.  Anything passed in will be added as \\'datagroup_$tag\\' with a value of 1",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "name": "dataAsset",
                    "type": "text",
                    "value": "@GetOrCreateRawAsset",
                    "required": false
                  },
                  {
                    "name": "datagroups",
                    "type": "text",
                    "value": "!feed.dataGroups",
                    "required": false
                  },
                  {
                    "name": "tagZone",
                    "type": "text",
                    "value": "true",
                    "required": false
                  }
                ],
                "engineMeta": {
                  "spark": "DataAssetSteps.tagDataAssetWithDataGroups",
                  "pkg": "com.acxiom.datalake.pipeline.steps"
                },
                "creationDate": "2019-06-04T19:45:31.112Z",
                "modifiedDate": "2019-06-04T19:45:31.112Z",
                "stepId": "04622d9c-7708-557e-b4c8-1268131d6f77"
              }
            ],
            "id": "ff8428d0-879c-11e9-8b4f-9b89e27b8b39",
            "creationDate": "2019-06-05T14:19:58.305Z",
            "modifiedDate": "2019-06-05T15:59:42.200Z"
          }
        ],
        "id": "027f8580-8800-11e9-8b4f-9b89e27b8b39",
        "creationDate": "2019-06-06T02:08:43.483Z",
        "modifiedDate": "2019-06-06T22:19:24.136Z",
        "requiredParameters": [
          "stsUrl",
          "msApiUrl",
          "ignoreSSL",
          "apiKey",
          "apiSecret",
          "tenantId",
          "coreId",
          "servicesUrl",
          "dataCollectionId"
        ]
      },
      {
        "_id": "5d09106b72578d23ebac03af",
        "name": "avarho_test",
        "globals": {
          
        },
        "executions": [
          {
            "id": "Create Modified DataAsset",
            "pipelineIds": [
              "1f37f550-9146-11e9-ac99-49159f6e60a9"
            ],
            "pipelines": [
              
            ],
            "globals": {
              
            }
          }
        ],
        "pipelines": [
          {
            "_id": "5d0804c48c5e44aa7f933ec5",
            "name": "avarho Load Test",
            "steps": [
              {
                "id": "SFTP FileManager",
                "displayName": "Create SFTP FileManager",
                "description": "Simple function to generate the SFTPFileManager for the remote SFTP file system",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "hostName",
                    "required": false,
                    "value": "!sftp_host"
                  },
                  {
                    "name": "username",
                    "type": "text",
                    "value": "!sftp_user",
                    "required": false
                  },
                  {
                    "name": "password",
                    "type": "text",
                    "value": "!sftp_pass",
                    "required": false
                  },
                  {
                    "name": "port",
                    "type": "integer",
                    "value": "!sftp_port",
                    "required": false
                  },
                  {
                    "type": "boolean",
                    "name": "strictHostChecking",
                    "required": false,
                    "value": "false"
                  }
                ],
                "engineMeta": {
                  "spark": "SFTPSteps.createFileManager",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T19:10:59.281Z",
                "modifiedDate": "2019-06-07T00:10:31.084Z",
                "stepId": "9d467cb0-8b3d-40a0-9ccd-9cf8c5b6cb38",
                "executeIfEmpty": "",
                "nextStepId": "HDFS FileManager"
              },
              {
                "id": "HDFS FileManager",
                "displayName": "Create HDFS FileManager",
                "description": "Simple function to generate the HDFSFileManager for the local HDFS file system",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.createFileManager",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T18:54:44.836Z",
                "modifiedDate": "2019-05-28T18:54:44.836Z",
                "stepId": "e4dad367-a506-5afd-86c0-82c2cf5cd15c",
                "nextStepId": "Download File"
              },
              {
                "id": "Download File",
                "displayName": "Buffered file copy",
                "description": "Copy the contents of the source path to the destination path using full buffer sizes. This function will call connect on both FileManagers.",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "srcFS",
                    "required": false,
                    "value": "@SFTP FileManager"
                  },
                  {
                    "type": "text",
                    "name": "srcPath",
                    "required": false,
                    "value": "!sftp_input_path"
                  },
                  {
                    "type": "text",
                    "name": "destFS",
                    "required": false,
                    "value": "@HDFS FileManager"
                  },
                  {
                    "type": "text",
                    "name": "destPath",
                    "required": false,
                    "value": "!landing_path"
                  },
                  {
                    "type": "integer",
                    "name": "inputBufferSize",
                    "required": false,
                    "value": "!input_buffer_size"
                  },
                  {
                    "type": "integer",
                    "name": "outputBufferSize",
                    "required": false,
                    "value": "!output_buffer_size"
                  },
                  {
                    "type": "integer",
                    "name": "copyBufferSize",
                    "required": false,
                    "value": "!read_buffer_size"
                  }
                ],
                "engineMeta": {
                  "spark": "FileManagerSteps.copy",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T18:54:44.883Z",
                "modifiedDate": "2019-06-07T00:11:00.761Z",
                "stepId": "f5a24db0-e91b-5c88-8e67-ab5cff09c883",
                "nextStepId": "Disconnect SFTP FileManager"
              },
              {
                "id": "Disconnect SFTP FileManager",
                "displayName": "Disconnect a FileManager",
                "description": "Disconnects a FileManager from the underlying file system",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "fileManager",
                    "required": false,
                    "value": "@SFTP FileManager"
                  }
                ],
                "engineMeta": {
                  "spark": "FileManagerSteps.disconnectFileManager",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T18:54:44.887Z",
                "modifiedDate": "2019-05-28T18:54:44.887Z",
                "stepId": "3d1e8519-690c-55f0-bd05-1e7b97fb6633",
                "nextStepId": "LoadDataframe"
              },
              {
                "id": "LoadDataframe",
                "displayName": "Load DataFrame from HDFS path",
                "description": "This step will read a dataFrame from the given HDFS path",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "path",
                    "required": false,
                    "value": "!landing_path"
                  },
                  {
                    "type": "object",
                    "name": "options",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameReaderOptions",
                    "value": {
                      "format": "csv",
                      "options": {
                        "header": "true",
                        "delimiter": ","
                      },
                      "schema": {
                        "attributes": [
                          
                        ]
                      }
                    }
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.readFromPath"
                },
                "creationDate": "2019-04-04T14:24:22.875Z",
                "modifiedDate": "2019-04-04T14:24:22.875Z",
                "stepId": "87db259d-606e-46eb-b723-82923349640f",
                "nextStepId": "Write DataFrame to HDFS"
              },
              {
                "id": "Write DataFrame to HDFS",
                "displayName": "Write DataFrame to HDFS",
                "description": "This step will write a dataFrame in a given format to HDFS",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "dataFrame",
                    "required": false,
                    "value": "@LoadDataFrame"
                  },
                  {
                    "type": "text",
                    "name": "path",
                    "required": false,
                    "value": "/raw/avarho_parquet"
                  },
                  {
                    "type": "object",
                    "name": "options",
                    "required": false,
                    "className": "com.acxiom.pipeline.steps.DataFrameWriterOptions"
                  }
                ],
                "engineMeta": {
                  "spark": "HDFSSteps.writeToPath"
                },
                "creationDate": "2019-04-04T14:24:22.889Z",
                "modifiedDate": "2019-04-04T14:24:22.889Z",
                "stepId": "0a296858-e8b7-43dd-9f55-88d00a7cd8fa",
                "nextStepId": "Disconnect HDFS"
              },
              {
                "id": "Disconnect HDFS",
                "displayName": "Disconnect a FileManager",
                "description": "Disconnects a FileManager from the underlying file system",
                "type": "Pipeline",
                "category": "InputOutput",
                "params": [
                  {
                    "type": "text",
                    "name": "fileManager",
                    "required": false,
                    "value": "@HDFS FileManager"
                  }
                ],
                "engineMeta": {
                  "spark": "FileManagerSteps.disconnectFileManager",
                  "pkg": "com.acxiom.pipeline.steps"
                },
                "creationDate": "2019-05-28T18:54:44.887Z",
                "modifiedDate": "2019-05-28T18:54:44.887Z",
                "stepId": "3d1e8519-690c-55f0-bd05-1e7b97fb6633"
              }
            ],
            "id": "1f37f550-9146-11e9-ac99-49159f6e60a9",
            "creationDate": "2019-06-17T21:23:16.903Z",
            "modifiedDate": "2019-06-18T16:19:23.396Z"
          }
        ],
        "id": "a7511d00-91e5-11e9-a0f0-e1ddb30bb970",
        "creationDate": "2019-06-18T16:25:15.219Z",
        "modifiedDate": "2019-06-18T16:41:33.531Z",
        "requiredParameters": [
          "sftp_host",
          "sftp_user",
          "sftp_pass",
          "sftp_port",
          "sftp_input_path",
          "landing_path",
          "input_buffer_size",
          "output_buffer_size",
          "read_buffer_size",
          "write_path"
        ],
        "sparkConf": {
          "kryoClasses": [
            "org.apache.hadoop.io.LongWritable",
            "org.apacje.http.client.entity.UrlEncodedFormEntity"
          ],
          "setOptions": [
            {
              "name": "spark.hadoop.io.compression.codecs",
              "value": "org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.Lz4Codec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.GzipCodec"
            },
            {
              "name": "applicationConfigurationLoader",
              "value": "com.acxiom.pipeline.fs.HDFSFileManager"
            }
          ]
        },
        "stepPackages": [
          "com.acxiom.pipeline.steps",
          "com.acxiom.pipeline"
        ]
      }
    ]
  }